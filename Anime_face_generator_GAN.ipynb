{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919a2147-243a-44af-8a61-3ecb53f04a02",
   "metadata": {},
   "source": [
    "## Downloading dataset from Kaggle\n",
    "\n",
    "To use Kaggle dataset, it's good to use [Kaggle Official API](https://github.com/Kaggle/kaggle-api). To be able to use this API properly, you need API credential which you can get by following some simple steps.\n",
    "\n",
    "1. Sign in to  [https://kaggle.com/](https://kaggle.com),  then click on your profile picture on the top right and select \" Account\" from the menu.\n",
    "\n",
    "2. Scroll down to the \"API\" section and click \"Create New API Token\". This will download a file `kaggle.json` with the following contents:\n",
    "\n",
    "```\n",
    "{\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}\n",
    "```\n",
    "\n",
    "3. Save the generated file `kaggle.json` in the same directory as this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb964d-6736-45d2-955d-15d839ba5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3639a-4982-4aba-afb4-94969aa0d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.dataset_download_files('splcher/animefacedataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c01d1-91f2-4dfe-9d40-bf501375477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zf = ZipFile('animefacedataset.zip')\n",
    "zf.extractall()\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14186-a778-477c-b58f-0fca540ad28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36cfe-2b25-4b1a-b95d-af5b9380a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "# Stats for normalizing images\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \n",
    "\n",
    "DATA_DIR = './animefacedataset'\n",
    "\n",
    "train_ds = ImageFolder(DATA_DIR, transform = T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.CenterCrop(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "]))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4386c4-6865-4dc2-8cb1-e79be38f9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we normalize images at the time of creating training dataset,\n",
    "# if we want to visualize them, we have to denormalize them \n",
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1,2,0))\n",
    "    \n",
    "    \n",
    "def show_batch(dl, nmax =64):\n",
    "    for images,_ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48fdd5-f79e-43b3-b9f3-1f1a2e51babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81c13e-463a-420b-bea0-88a542c371a2",
   "metadata": {},
   "source": [
    "As pyTorch gives facility of running code on CPU as well as on GPU, we can move our data to either of them. For that we can find if a GPU is available or not and then move data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25cb30-5cb6-4303-a5d9-7916753e0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    '''\n",
    "    Selects GPU if availabel otervise selects CPU.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    \n",
    "def to_device(data, device):\n",
    "    '''\n",
    "    Move tensors and networks to selected device\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(i,device) for i in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb94448-4a59-497c-abfc-f89fe23628c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple class to move data to selected device.\n",
    "class DeviceDataLoader():\n",
    "    '''\n",
    "    Wrap a dataloader to move data to a device\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Get a batch of data after moving it to a selected device\n",
    "        \n",
    "        '''\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31002d-abaa-4b33-b250-fbd4095996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b288be9-40e4-405c-822b-05cdb911a955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecee192-2bc3-4878-8a6c-37e2e6df08c9",
   "metadata": {},
   "source": [
    "GAN is divided into main two parts, Discriminator and Generator. Function of later one is to create images and the function of the formar one is to differentiate between real and generated image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13c72e-0332-457b-b38a-cada5fe6314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5cb9a-2466-4562-8c4c-af8a1f4b5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128 \n",
    "\n",
    "# In simple term, latent size is number of space dimention\n",
    "# where each dimention learn different feature of \"to be generated\"\n",
    "# image. In this case, features like eyes, hair, nose, etc.\n",
    "\n",
    "generator = nn.Sequential(\n",
    "    # input: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # output: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # output: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # output: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # output: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh()\n",
    "    # output: 3 x 64 x 64\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b7933-fb3f-4fad-a657-62c009127947",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    # input: 3 x 64 x 64\n",
    "\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # output: 64 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # output: 128 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # output: 256 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # output: 512 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # output: 1 x 1 x 1\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid()) \n",
    "\n",
    "    # Sigmoid as a last layer because the function of the discriminator \n",
    "    # is to find if the image is real or fake. For that binary classification\n",
    "    # sigmoid gives good result.\n",
    "\n",
    "    \n",
    "# Research has showed that LeakyRELU is better for discriminator and RELU is better\n",
    "# for generator. But if you want you can try and experiment using different activation \n",
    "# function and see if it gives you better result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141b517-917c-43ee-8766-d91acec8a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)\n",
    "generator = to_device(generator, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdf953-b1fe-441f-8dd6-218e269599a0",
   "metadata": {},
   "source": [
    "## Discriminator training \n",
    "\n",
    "Main thing you have to understand in GAN is how the discriminator and generator is trained. To train discriminator first of all real images will be given as input and loss will be calculated using `binary_cross_entropy` function.\n",
    "Then some fake images from generator will be given as input and again loss will be calculated. At the end gradient will be calculaed with the combine loss of real and fake image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724f848-28b6-40b7-9e17-d1857ccb0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_discriminator):\n",
    "    # Set discriminator gradient to zero\n",
    "    opt_discriminator.zero_grad()\n",
    "    \n",
    "    # Pass real images through descriminator\n",
    "    real_preds = discriminator(real_images)\n",
    "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "    \n",
    "    # Generate fake images and pass them through discriminator\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "    \n",
    "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device) # As they are fake images, target is \"0\"\n",
    "    fake_preds = discriminator(fake_images)\n",
    "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "    \n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_discriminator.step()\n",
    "    return loss.item(), real_score, fake_score   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e308c60-4463-4a74-a727-f8768f09f985",
   "metadata": {},
   "source": [
    "## Generator training\n",
    "\n",
    "Generator trainig is a bit trickier than discriminator, because for discriminator we are giving images from generator as an input and from the loss we can train it. But here we have to generate image so the trick we have to use is that, we have  to use discriminator as a part to get the loss function of the generator. \n",
    "\n",
    "The trick we have to use is that, even though we are using fake images for discrimonator, we have to give one as a target. For discriminator, we took one as a target for real images and zero for fake images. But here we want that generator produces images which look like they are real. So if we take one as target, discriminator will give high loss for images which are not close to real images and thus generator will learn how to produce images which are close to real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155287e0-2202-4c22-ae53-ec49f98467c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_generator):\n",
    "    # Set generator gradients to zero\n",
    "    opt_generator.zero_grad()\n",
    "    \n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "    \n",
    "    # Try to fool the discriminator \n",
    "    preds = discriminator(fake_images)\n",
    "    targets = torch.ones(batch_size, 1, device=device)\n",
    "    loss = F.binary_cross_entropy(preds, targets)\n",
    "    \n",
    "    # Update generator weights\n",
    "    loss.backward()\n",
    "    opt_generator.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f8ec-fe56-4395-947a-170e8315aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed1c7e-9e13-4641-9e15-76246ccc1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(index, latent_tensors, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_img_name = f'generated_image_no_{index}.png'\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_img_name), nrow=8)\n",
    "    \n",
    "    print('Saving', fake_img_name)\n",
    "    \n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1,2,0))\n",
    "        \n",
    "        # Detach is used to tell pytorch that for this tensor we dont need other \n",
    "        # information like gradient. We just need its value for calculatoin. \n",
    "        # So it can be removed from graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ff3da-4000-4975-b00a-2e1aa694911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe72d53-8a89-4838-b189-9c829dbe20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(0, fixed_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cc227-be4e-48bb-bdfc-461e41023828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d9725-2258-42dc-9437-1dd328499b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, start_idx=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Losses and scores\n",
    "    losses_generator = []\n",
    "    losses_discriminator = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    # Create optimizers\n",
    "    opt_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    opt_generator = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for real_images, _ in tqdm(train_dl):\n",
    "            # Train discriminator and generator\n",
    "            loss_discriminator, real_score, fake_score = train_discriminator(real_images, opt_discriminator)\n",
    "            loss_generator = train_generator(opt_generator)\n",
    "\n",
    "        #Train losses and scores\n",
    "        losses_generator.append(loss_generator)\n",
    "        losses_discriminator.append(loss_discriminator)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "\n",
    "        # Log losses and scores \n",
    "        print(f\"Epoch No.: {epoch+1}/{epochs}, loss_generator: {loss_generator:.3f}, loss_discriminator: {loss_discriminator: .3f}, real_score: {real_score: .3f}, fake_score: {fake_score: .3f}\")\n",
    "\n",
    "        #Save generated images\n",
    "        save_images(index=start_idx, latent_tensors=fixed_latent, show=False)\n",
    "            \n",
    "    return losses_generator, losses_discriminator, real_scores, fake_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6cb4d-73b9-40af-9cb1-b8fe7f2f0fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "epochs = 200\n",
    "\n",
    "history = fit(epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352192e-f7ed-4de4-9685-1138c63dadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_generator, losses_discriminator, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1424a4-c6a5-4ede-b536-d09786ad6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discriminator and generator models.\n",
    "torch.save(generator.state_dict(), 'Generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'Discriminator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfee160-7382-41e9-978e-d905e14a7188",
   "metadata": {},
   "source": [
    "Let's visulaize which kind of anime faces has been generated by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f63bdd-102d-407a-b6c1-3f3690d30f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('./generated/generated_image_no_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decd1b6-643f-475e-9834-94b70f057d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated_image_no_10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17179f-68b1-4b3d-a5d5-a72def2b30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated_image_no_20.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b829157-7788-4a93-9c94-8778af108246",
   "metadata": {},
   "source": [
    "It's good to see that trained network is giving good results. It's better to see how those images evolve from epoch 1. We can make a video out of all generated images using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db618bf-e392-4a51-9782-0c87d9a82151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vid_fname = 'Anime_face_GAN.avi'\n",
    "\n",
    "\n",
    "img_dir = []\n",
    "for img in os.listdir(sample_dir):\n",
    "    if img.startswith('generated'):\n",
    "        img_dir.append(os.path.join(sample_dir, img))\n",
    "\n",
    "img_dir.sort()\n",
    "\n",
    "\n",
    "vid = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
    "\n",
    "[out.write(cv2.imread(img_name)) for img_name in files]\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07014f8-d4b0-4f0f-b1e5-111f51a47e7d",
   "metadata": {},
   "source": [
    "We can also visualize how losses changes overtime, which helps debugging the training process. For GAN, we expect the generator's loss to reduce overtime without the discriminator loss getting too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c1879-37de-4e46-98a9-dae80eb9c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_discriminator, '-')\n",
    "plt.plot(losses_generator, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c069f-a62b-4153-bd59-0d77d6d1ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_scores, '-')\n",
    "plt.plot(fake_scores, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.title('Scores');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
